{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"LangChain est une composante cl√© dans la cr√©ation d'applications avec des LLM.\",\n",
    "    \"La m√©thode BM25 permet d'am√©liorer la qualit√© de la recherche documentaire.\",\n",
    "    \"Avec FAISS, les r√©sultats sont plus pertinents gr√¢ce √† l'analyse s√©mantique.\",\n",
    "    \"Le retrieval hybride est souvent combin√© avec des approches lexicales pour des performances accrues.\",\n",
    "    \"L'int√©gration de embeddings dans un pipeline RAG est d√©sormais plus simple gr√¢ce √† LangChain.\",\n",
    "    \"Les syst√®mes de vector search utilisent des embeddings pour repr√©senter le texte num√©riquement.\",\n",
    "    \"FAISS am√©liore la pr√©cision de la recherche en comprenant le sens des requ√™tes.\",\n",
    "    \"BM25 fonctionne tr√®s bien en local, m√™me sans infrastructure cloud.\",\n",
    "    \"Gr√¢ce √† LangChain, on peut interroger efficacement une base documentaire complexe.\",\n",
    "    \"La combinaison de BM25 et FAISS est tr√®s utilis√©e dans les syst√®mes RAG modernes.\",\n",
    "    \"LLM est une composante cl√© dans la cr√©ation d'applications avec des LLM.\",\n",
    "    \"La m√©thode hybrid methods permet d'am√©liorer la qualit√© de la recherche documentaire.\",\n",
    "    \"Avec dense retrieval, les r√©sultats sont plus pertinents gr√¢ce √† l'analyse s√©mantique.\",\n",
    "    \"vector search est souvent combin√© avec des approches lexicales pour des performances accrues.\",\n",
    "    \"L'int√©gration de retrieval dans un pipeline RAG est d√©sormais plus simple gr√¢ce √† LangChain.\",\n",
    "    \"Les syst√®mes de metadata utilisent des embeddings pour repr√©senter le texte num√©riquement.\",\n",
    "    \"question answering am√©liore la pr√©cision de la recherche en comprenant le sens des requ√™tes.\",\n",
    "    \"open source fonctionne tr√®s bien en local, m√™me sans infrastructure cloud.\",\n",
    "    \"Gr√¢ce √† document processing, on peut interroger efficacement une base documentaire complexe.\",\n",
    "    \"La combinaison de FAISS et embeddings est tr√®s utilis√©e dans les syst√®mes RAG modernes.\",\n",
    "    \"LangChain est souvent combin√© avec FAISS dans les pipelines IA modernes.\",\n",
    "    \"BM25 est toujours utilis√© dans les moteurs de recherche traditionnels.\",\n",
    "    \"Les embeddings permettent une compr√©hension s√©mantique fine.\",\n",
    "    \"Une base de connaissance bien structur√©e am√©liore le retrieval.\",\n",
    "    \"Chunker les documents aide les retrievers √† mieux cibler les informations.\",\n",
    "    \"Les LLMs comme Mistral ou LLaMA profitent d‚Äôun bon contexte via retrieval.\",\n",
    "    \"Le vector store est essentiel pour la recherche dense.\",\n",
    "    \"FAISS et Chroma sont les solutions les plus courantes en local.\",\n",
    "    \"LangChain propose une API unifi√©e pour tous les types de retrievers.\",\n",
    "    \"Le format des documents (PDF, Markdown, HTML) influence le chunking.\",\n",
    "    \"BM25 reste performant sur les corpus simples ou tr√®s structur√©s.\",\n",
    "    \"Le reranking am√©liore la qualit√© finale des documents propos√©s au LLM.\",\n",
    "    \"Certaines approches hybrides utilisent des pond√©rations dynamiques.\",\n",
    "    \"Weaviate permet de combiner recherche vectorielle et filtrage structur√©.\",\n",
    "    \"Avec Ollama, les LLMs peuvent tourner localement sans cloud.\",\n",
    "    \"Le co√ªt des embeddings d√©pend du mod√®le utilis√© (MiniLM, MPNet‚Ä¶).\",\n",
    "    \"La qualit√© du prompt est influenc√©e par le contexte fourni par le retriever.\",\n",
    "    \"Un bon split des documents augmente le recall.\",\n",
    "    \"Les m√©triques comme le MRR ou Recall@k permettent d‚Äô√©valuer un retriever.\",\n",
    "    \"L‚Äôinfrastructure locale doit √™tre optimis√©e pour charger rapidement l‚Äôindex.\",\n",
    "    \"L‚Äôajout de m√©tadonn√©es enrichit le filtrage contextuel dans les retrievers.\",\n",
    "    \"LangChain permet de tester rapidement plusieurs strat√©gies de retrieval.\",\n",
    "    \"Une pipeline RAG efficace passe par un tuning pr√©cis du retriever.\",\n",
    "    \"Le choix de l‚Äôembedding model impacte directement la pertinence des r√©sultats.\",\n",
    "    \"Il est utile d‚Äôindexer les sources des documents pour l‚Äôauditabilit√©.\",\n",
    "    \"Certaines approches utilisent des embeddings multi-vecteurs par chunk.\",\n",
    "    \"FAISS HNSW est plus performant pour des recherches approximatives rapides.\",\n",
    "    \"La vectorisation peut se faire offline pour gagner en performance.\",\n",
    "    \"LangChain offre aussi un support pour OpenSearch et Qdrant.\",\n",
    "    \"Un LLM avec peu de contexte aura du mal √† r√©pondre pr√©cis√©ment.\",\n",
    "    \"Il faut ajuster la taille et le recouvrement des chunks pour chaque cas d‚Äôusage.\"\n",
    "]\n",
    "documents = [Document(page_content=t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "docs_split = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models = {\n",
    "    \"miniLM\": HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "    \"mpnet\": HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievers = {}\n",
    "\n",
    "for name, embedding in embedding_models.items():\n",
    "    vectorstore = FAISS.from_documents(docs_split, embedding)\n",
    "    retrievers[f\"FAISS ({name})\"] = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(docs_split)\n",
    "bm25_retriever.k = 3\n",
    "retrievers[\"BM25\"] = bm25_retriever\n",
    "\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, retrievers[\"FAISS (miniLM)\"]],\n",
    "    weights=[0.5, 0.5]\n",
    ")\n",
    "retrievers[\"Hybrid (BM25 + FAISS-miniLM)\"] = hybrid_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_query(query: str, top_k: int = 2):\n",
    "    print(f\"\\nüîç Question : {query}\")\n",
    "    for name, retriever in retrievers.items():\n",
    "        print(f\"\\n‚û° M√©thode : {name}\")\n",
    "        results = retriever.invoke(query)\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"{i+1}. {doc.page_content}\")\n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Question : Comment fonctionne la recherche vectorielle ?\n",
      "\n",
      "‚û° M√©thode : FAISS (miniLM)\n",
      "1. Le vector store est essentiel pour la recherche dense.\n",
      "2. Weaviate permet de combiner recherche vectorielle et filtrage structur√©.\n",
      "3. La vectorisation peut se faire offline pour gagner en performance.\n",
      "\n",
      "‚û° M√©thode : FAISS (mpnet)\n",
      "1. La vectorisation peut se faire offline pour gagner en performance.\n",
      "2. Weaviate permet de combiner recherche vectorielle et filtrage structur√©.\n",
      "3. Le vector store est essentiel pour la recherche dense.\n",
      "\n",
      "‚û° M√©thode : BM25\n",
      "1. Weaviate permet de combiner recherche vectorielle et filtrage structur√©.\n",
      "2. La m√©thode BM25 permet d'am√©liorer la qualit√© de la recherche documentaire.\n",
      "3. La m√©thode hybrid methods permet d'am√©liorer la qualit√© de la recherche documentaire.\n",
      "\n",
      "‚û° M√©thode : Hybrid (BM25 + FAISS-miniLM)\n",
      "1. Weaviate permet de combiner recherche vectorielle et filtrage structur√©.\n",
      "2. Le vector store est essentiel pour la recherche dense.\n",
      "3. La m√©thode BM25 permet d'am√©liorer la qualit√© de la recherche documentaire.\n",
      "4. La m√©thode hybrid methods permet d'am√©liorer la qualit√© de la recherche documentaire.\n",
      "5. La vectorisation peut se faire offline pour gagner en performance.\n",
      "\n",
      "==================================================\n",
      "\n",
      "üîç Question : Qu'est-ce qu'un mod√®le d'embedding ?\n",
      "\n",
      "‚û° M√©thode : FAISS (miniLM)\n",
      "1. Le co√ªt des embeddings d√©pend du mod√®le utilis√© (MiniLM, MPNet‚Ä¶).\n",
      "2. Le choix de l‚Äôembedding model impacte directement la pertinence des r√©sultats.\n",
      "3. Les embeddings permettent une compr√©hension s√©mantique fine.\n",
      "\n",
      "‚û° M√©thode : FAISS (mpnet)\n",
      "1. Le choix de l‚Äôembedding model impacte directement la pertinence des r√©sultats.\n",
      "2. Les embeddings permettent une compr√©hension s√©mantique fine.\n",
      "3. Le co√ªt des embeddings d√©pend du mod√®le utilis√© (MiniLM, MPNet‚Ä¶).\n",
      "\n",
      "‚û° M√©thode : BM25\n",
      "1. Le co√ªt des embeddings d√©pend du mod√®le utilis√© (MiniLM, MPNet‚Ä¶).\n",
      "2. Un LLM avec peu de contexte aura du mal √† r√©pondre pr√©cis√©ment.\n",
      "3. Il faut ajuster la taille et le recouvrement des chunks pour chaque cas d‚Äôusage.\n",
      "\n",
      "‚û° M√©thode : Hybrid (BM25 + FAISS-miniLM)\n",
      "1. Le co√ªt des embeddings d√©pend du mod√®le utilis√© (MiniLM, MPNet‚Ä¶).\n",
      "2. Un LLM avec peu de contexte aura du mal √† r√©pondre pr√©cis√©ment.\n",
      "3. Le choix de l‚Äôembedding model impacte directement la pertinence des r√©sultats.\n",
      "4. Il faut ajuster la taille et le recouvrement des chunks pour chaque cas d‚Äôusage.\n",
      "5. Les embeddings permettent une compr√©hension s√©mantique fine.\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "test_query(\"Comment fonctionne la recherche vectorielle ?\")\n",
    "test_query(\"Qu'est-ce qu'un mod√®le d'embedding ?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-streamlit-app (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
