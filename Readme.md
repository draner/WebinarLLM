# Application RAG avec Ollama et Streamlit

## FonctionnalitÃ©s

- ğŸ¤– Utilisation de LLMs locaux via Ollama
- ğŸ“š Base de connaissance vectorielle avec ChromaDB
- ğŸ“„ Support pour l'import de documents PDF, TXT et CSV
- ğŸ” Recherche sÃ©mantique sur les documents importÃ©s
- ğŸ’¬ Interface de chat interactive
- ğŸ“Š Gestion et visualisation des documents

## PrÃ©requis

- Python 3.11 ou supÃ©rieur
- [Ollama](https://github.com/ollama/ollama) installÃ© et en cours d'exÃ©cution

## Installation

### 1. Cloner le dÃ©pÃ´t

```bash
git clone https://github.com/draner/WebinarLLM.git
cd WebinarLLM
```

### 2. Configuration de l'environnement

Installez les dÃ©pendances Ã  l'aide de uv:

```bash
# Installation de uv si vous ne l'avez pas dÃ©jÃ 
pip install uv

# Installation des dÃ©pendances
uv sync
```

Alternatively, vous pouvez aussi installer avec pip:

```bash
uv pip install -e .
```

### 3. PrÃ©paration d'Ollama

Assurez-vous qu'Ollama est en cours d'exÃ©cution et que vous avez tÃ©lÃ©chargÃ© les modÃ¨les nÃ©cessaires:

```bash
# TÃ©lÃ©charger un modÃ¨le de chat (si ce n'est pas dÃ©jÃ  fait)
ollama pull mistral

# TÃ©lÃ©charger un modÃ¨le d'embeddings (si ce n'est pas dÃ©jÃ  fait)
ollama pull nomic-embed-text
```

## Utilisation

### Activer l'environnement virtuel
Selon votre systÃ¨me d'exploitation :
```bash
# Linux / MacOS :
source .venv/bin/activate
# Windows :
.venv\Scripts\activate
```

### Lancer l'application

```bash
streamlit run rag_app/app.py
```

## Structure du projet

```sh
rag-streamlit-app/
â”œâ”€â”€ pyproject.toml       # Configuration du projet et dÃ©pendances
â”œâ”€â”€ README.md            # Ce fichier
â””â”€â”€ rag_app/             # Code source de l'application
    â””â”€â”€ app.py           # Application Streamlit principale
```
